{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen \n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import csv\n",
    "import urllib.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'https://www.mystore411.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen('https://www.mystore411.com')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "urls = []\n",
    "for link in bs.find('div', {'id':'breadcrumb'}).find_all('a', href=re.compile('^(/store/)((?!:).)*$')):\n",
    "    if 'href' in link.attrs:\n",
    "        letters = link.attrs['href']\n",
    "        urls.append(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_urls = ['{}{}'.format(base,i) for i in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = []\n",
    "for each in my_urls:\n",
    "    html2 = urlopen(each)\n",
    "    soup = BeautifulSoup(html2, \"lxml\" )\n",
    "    newnewurl = []\n",
    "    for ul in soup.find_all('ol', class_='col2 d_store_list'):\n",
    "        for li in ul.find_all('li'):\n",
    "            a = li.find('a')\n",
    "            newnewurl = a['href']\n",
    "            stores.append(newnewurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stores = ['{}{}'.format(base,i) for i in stores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_Canada_list = [s for s in my_stores if \"/Canada/\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stores = [s for s in my_stores if \"/Canada/\" not in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stores2 = [re.sub(r'\\s+', '%20', x) for x in other_stores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_Canada_list = []\n",
    "for w in other_stores2:\n",
    "    html3 = urlopen(w)\n",
    "    soup2 = BeautifulSoup(html3, \"lxml\")\n",
    "    urlCanada = []\n",
    "    for ul in soup2.find_all('ul', class_='sidemenu salmon'):\n",
    "        for li in ul.find_all('li'): \n",
    "            a = li.find('a')\n",
    "            urlCanada = a['href'] \n",
    "            second_Canada_list.append(urlCanada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_second_Canada_list = ['{}{}'.format(base,i) for i in second_Canada_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_final_second_Canada_list = [s for s in my_second_Canada_list if \"/Canada/\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_Canada_list = my_final_second_Canada_list + first_Canada_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinces = []\n",
    "for j in final_Canada_list:\n",
    "    html5 = urlopen(j)\n",
    "    soup4 = BeautifulSoup(html5, \"lxml\")\n",
    "    urlProv = []\n",
    "    for table in soup4.find_all('table', class_='table1'):\n",
    "        for a in table.find_all('a'):\n",
    "            urlProv = a['href']\n",
    "            provinces.append(urlProv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_provinces = ['{}{}'.format(base,i) for i in provinces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_provinces2 = [re.sub(r'\\s+', '%20', x) for x in my_provinces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_ready_for_scraping = [s for s in my_provinces2 if \"/view/\" in s]\n",
    "stores_the_last_step = [s for s in my_provinces2 if \"/view/\" not in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stores_the_last_step = [re.sub(r'\\s+', '%20', x) for x in stores_the_last_step]\n",
    "my_stores_the_last_step2 = [str(m) for m in my_stores_the_last_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_stores = []\n",
    "for q in my_stores_the_last_step2:\n",
    "    html6 = urlopen(q)\n",
    "    soup5 = BeautifulSoup(html6, \"lxml\")\n",
    "    last = []\n",
    "    for table in soup5.find_all('table', class_='table1'):\n",
    "        for a in table.find_all('a'):\n",
    "            last = a['href']\n",
    "            last_stores.append(last)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_last_stores = ['{}{}'.format(base,i) for i in last_stores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_ready_for_scraping2 = [s for s in my_last_stores if \"/view/\" in s]\n",
    "stores_the_last_step2 = [s for s in my_last_stores if \"/view/\" not in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_ready_for_scraping3 = stores_ready_for_scraping + stores_ready_for_scraping2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_stores2 = []\n",
    "for c in stores_the_last_step2:\n",
    "    html7 = urlopen(c)\n",
    "    soup6 = BeautifulSoup(html7, \"lxml\")\n",
    "    last2 = []\n",
    "    for table in soup6.find_all('table', class_='table1'):\n",
    "        for a in table.find_all('a'):\n",
    "            last2 = a['href']\n",
    "            last_stores2.append(last2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_last_stores2 = ['{}{}'.format(base,i) for i in last_stores2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_ready_for_scraping4 = [s for s in my_last_stores2 if \"/view/\" in s]\n",
    "stores_the_last_step3 = [s for s in my_last_stores2 if \"/view/\" not in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_ready_for_scraping5 = stores_ready_for_scraping3 + stores_ready_for_scraping4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_stores3 = []\n",
    "for r in stores_the_last_step3:\n",
    "    html8 = urlopen(r)\n",
    "    soup7 = BeautifulSoup(html8, \"lxml\")\n",
    "    last3 = []\n",
    "    for table in soup7.find_all('table', class_='table1'):\n",
    "        for a in table.find_all('a'):\n",
    "            last3 = a['href']\n",
    "            last_stores3.append(last3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_last_stores3 = ['{}{}'.format(base,i) for i in last_stores3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_ready_for_scraping6 = [s for s in my_last_stores3 if \"/view/\" in s]\n",
    "stores_the_last_step4 = [s for s in my_last_stores3 if \"/view/\" not in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_ready_for_scraping7 = stores_ready_for_scraping5 + stores_ready_for_scraping6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_stores4 = []\n",
    "for u in stores_the_last_step4:\n",
    "    html9 = urlopen(u)\n",
    "    soup8 = BeautifulSoup(html9, \"lxml\")\n",
    "    last4 = []\n",
    "    for table in soup8.find_all('table', class_='table1'):\n",
    "        for a in table.find_all('a'):\n",
    "            last4 = a['href']\n",
    "            last_stores4.append(last4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_last_stores4 = ['{}{}'.format(base,i) for i in last_stores4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_ready_for_scraping8 = [s for s in my_last_stores4 if \"/view/\" in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_ready_for_scraping9 = stores_ready_for_scraping7 + stores_ready_for_scraping8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores=[]\n",
    "addresses=[]\n",
    "cities=[]\n",
    "provincies=[]\n",
    "postalcodes=[]\n",
    "\n",
    "for n in stores_ready_for_scraping9:\n",
    "    try:\n",
    "        html = urlopen(n).read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "    except (urllib.error.HTTPError,  urllib.error.URLError, UnicodeEncodeError):\n",
    "        print('Continue Scraping') \n",
    "        \n",
    "     \n",
    "    \n",
    "    for store in soup.find('span', itemprop = 'name'):\n",
    "        stores.append(store)\n",
    "        \n",
    "    \n",
    "    address = soup.find(itemprop = 'streetAddress').get_text()\n",
    "    addresses.append(address)\n",
    "    \n",
    "    \n",
    "    city = soup.find(itemprop = 'addressLocality').get_text()\n",
    "    cities.append(city)\n",
    "    \n",
    "    province = soup.find(itemprop = 'addressRegion').get_text()\n",
    "    provincies.append(province)\n",
    "    \n",
    "    postalcode = soup.find(itemprop = 'postalCode').get_text()\n",
    "    postalcodes.append(postalcode)\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open('Scraped_Data.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        writer.writerow((\"Store name\", \"Address\", \"City\", \"Province\", \"Postal Code\"))\n",
    "        writer.writerows(zip(stores, addresses, cities, provincies, postalcodes))   \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
